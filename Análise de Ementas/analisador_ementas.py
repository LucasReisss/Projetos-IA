# -*- coding: utf-8 -*-
"""Analisador_ementas.ipynb

Automatically generated by Colaboratory.

"""

import gspread
from oauth2client.service_account import ServiceAccountCredentials
import spacy 
from spacy.lang.pt.stop_words import STOP_WORDS
from string import punctuation

scope = ["https://spreadsheets.google.com/feeds","https://www.googleapis.com/auth/spreadsheets","https://www.googleapis.com/auth/drive.file","https://www.googleapis.com/auth/drive"]

creds = ServiceAccountCredentials.from_json_keyfile_name('creds.json', scope)

client = gspread.authorize(creds)

sheet = client.open("documento").sheet1

data = sheet.get_all_records()

row = sheet.row_values(5)
col = sheet.col_values(3)
print(row)

#trasferindo os textos da planilha para a variavel
texto = row
print (texto)

stopwords = list(STOP_WORDS)
stopwords

npl = spacy.load('en_core_web_sm')

doc = npl( str (texto))

#Lista de tokens
tokens = [token.text for token in doc]
print(tokens)

punctuation + '\n'

#Limpeza do texto
#Frequência que as palavras aparecem
frequencia_texto = {}
for word in doc:
  if word.text.lower() not in stopwords:
    if word.text.lower() not in punctuation:
      if word.text not in frequencia_texto.keys():
        frequencia_texto[word.text] = 1
      else:
        frequencia_texto[word.text] += 1

print (frequencia_texto)

#Frequência máxima
frequencia_maxima = max(frequencia_texto.values())

frequencia_maxima

#Normalizar Frequencia
for word in frequencia_texto.keys():
  frequencia_texto[word] = frequencia_texto[word]/frequencia_maxima

print(frequencia_texto)

#Tokenizar as frases
tokens_frases = [sent for sent in doc.sents]
print(tokens_frases)

#Calcular a pontuação das senteças
pontuacao_sentencas = {}
for sent in tokens_frases:
  for word in sent:
    if word.text.lower() in frequencia_texto.keys():
      if sent not in pontuacao_sentencas.keys():
        pontuacao_sentencas[sent] = frequencia_texto[word.text.lower()]
      else:
        pontuacao_sentencas[sent] += frequencia_texto[word.text.lower()]

pontuacao_sentencas

from heapq import  nlargest

#obter 30% das sentenças
select_length = int(len(tokens_frases)*0.3)
select_length

# Obeter o sumario
sumario = nlargest(select_length, pontuacao_sentencas, key = pontuacao_sentencas.get)

sumario

#combinar frases
sumario_final = [word.text for word in sumario]

sumario = ''.join(sumario_final)

print(texto)

print (sumario)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

row = sheet.row_values(2)
col = sheet.col_values(2)
print(row)

ementa = row
print(ementa)

# Número de n-gramas
n = 1

#Contador de n-gramas
counts = CountVectorizer(analyzer='word', ngram_range=(n,n))

#matriz de contagem
sm = str (sumario)
print(sm)
tex = str (ementa)
print(tex)

#matrix de contagem para os dois textos
n_gramas = counts.fit_transform([tex, sm ])

#dicionairo de n-gramas
dicionario_ng = counts.fit([tex, sm]).vocabulary_

#vetor de n-gamas
array_n_gramas = n_gramas.toarray()

print(array_n_gramas)

print (dicionario_ng)

# Calcular a interseção dos dois txtos
lista_intersecao = np.amin(n_gramas.toarray(), axis = 0)
lista_intersecao

#contar a soma do array
conta_intersecao = np.sum(lista_intersecao)
conta_intersecao

# Normalização
index_A = 0
conta_A = np.sum(n_gramas.toarray()[index_A])
conta_A

#Normaliza e mostra a medida do grau de similaridade
conta_intersecao/conta_A